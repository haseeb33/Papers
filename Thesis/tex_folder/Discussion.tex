\chapter{Discussion}
 Topic drifting and time-series topic popularity are the main aspects of this research and we compared these aspects for DTM and LDA. In this section, we discuss a few important points of both concepts as they apply to DTM and LDA.

\textbf{Topic Drifting}: There is no topic drifting for DTM trained on the "Twitter" dataset({Table \ref{table:topicDrifting}}), so the only important information which can be extracted from such datasets is the time-series topic popularity which can be extracted using LDA, thus avoiding the high cost of using DTM as the topic model.  For the "NeurIPS" dataset, the topic drifting increased as we increased the number of topics for the DTM model. All 90 topics have  $V_s$ greater than 120 words for the NeuIPS data in the 90-topic configuration(Table \ref{table:topicDrifting}), which means there was high topic drifting and this high topic drifting information provides rich insights into topic transitions. Therefore, if we specifically want to examine topic drifting over time in such datasets, DTM is a promising model to use; however, we must keep in mind that if our goal is topic popularity, then LDA is a far better option. From the same table, we also see the drifting in the topics extracted from the "News" dataset, but the vocabulary size is comparatively low for the higher number of topics. This means that we have topics drifting with such datasets, but it may not be as effective as we want. More subjective analysis based on your problem can help determine if DTM is a good option or not. One interesting insight should be mentioned here; DTM tends to forcefully find the topic transitions in some cases. For example, in the 30-topic configuration for the News dataset, topic 29 started with words like \textit{(archive, team, collection, sign, projects, machine, contains, lost, providing, comment, websites, collections, wayback)}, but the word distribution at the end was \textit{(travel, airport, flight, air, trip, passengers, flights, travelers, plane, airlines, united, airline, passenger)}. Looking at these distributions, we can say that DTM failed to extract the correct topic drifting over time for topic 29.

\textbf{Topic Popularity} We have $\gamma$ and $\theta$ distributions for DTM and LDA, respectively. Once the model is trained, we can extract these distributions for any document and these distributions provide the topics proportion for each document. We can estimate the number of documents for each topic using the method described in Section 3 for the LDA model and, with this time-series document estimation, we can construct a time-series topic popularity graph. Similarly, we can construct this graph for DTM topics. Thus, this fundamental information can be extracted using both models. Notably, the topic popularity extracted from DTM is a little vague because DTM topics have topic transition information embedded within the topics. For example, topic 4 shown in Figure \ref{fig:populationGraphs} has word distribution \textit{(retrieval, content, query, queries, text, relevance, documents, semantic, words, document, lda, relevant, collection, word, latent, topics)} at T= 0, which is about "Information retrieval from documents" and the word distribution at T= 30 is \textit{(topic, topics, document, lda, word, documents, words, latent, dirichlet, text, models, allocation, model, corpus, gibbs, modeling, blei)}, which is about "Document analysis with LDA". Similarly, DTM topic 33 was about "Language structure rules" in initial time slots and the theme of the topic was changed to "Question-answer reasoning" around the end. Therefore, if we are looking for the popularity graph of a topic "Information Retrieval", then LDA topic 12 is a more accurate option. Similarly, LDA topic 41 is more accurate if we want to see the popularity graph of the topic "Variational topic model LDA" because there is no topic drifting in LDA. The word distribution for LDA topic 12 is \textit{(word, words, language, sequence, recurrent, text, lstm, rnn, semantic, context, attention, table, vectors, embedding, sequences)} and for LDA 41 is \textit{(latent, inference, topic, sampling, mixture, variational, posterior, gibbs, dirichlet, topics, lda, markov, document, likelihood, prior, distributions)}. Because of the topic transition information embedded with DTM topics, it is not the best option for time-series topic popularity information.
