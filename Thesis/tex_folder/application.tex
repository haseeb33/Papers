\chapter{Use Case in Social Media Analysis}
News extraction from Twitter data is a hot topic. But can we extract much more than just news? The purpose of this experiment is to find, either news is the only information which can be extracted from Twitter data or it contains much more insights about real life events. So, we used proposed  method (LDA topic popularity) for analysis of Twitter’s raw content. After pre-processing of tweets data, we apply hashtag pooling and extract topics using LDA without modifying its core machinery. In the second part, estimated number of tweets per day and correlated top hashtags for each topic are calculated using day-hashtag pooling. Finally, time series topic popularity graphs are constructed for topic analysis. Interesting results of bursty news detection, topic popularity, people’s way to perceiving an event, real-life event’s transition over time and before \& after affects of a specific event were found.

\section{Twitter is Important}
Twitter is a very unique source of information where millions of users try to sum up an event, trend or their emotions into 140 characters. Diverse users of twitter freely express their thoughts which leads to many topics. Extracting trends from tweet’s data could be very handy to know and understand better about real-life events because of huge dataset available and people’s interest in it. The application area of twitter is vast including many useful domains such as real-time events detection \cite{sakaki2010earthquake}, sentiment predication analysis \cite{tumasjan2010predicting}, understanding public health opinions \cite{karami2018characterizing}, time series topic popularity variation \cite{fukuyama2018extracting} and it's comparison with traditional media \cite{zhao2011comparing}. Over 85\% of topics are headline news or persistent news in nature when tweets data is classified for trending topics  \cite{kwak2010twitter}. These topics aren't just only news but also contain reasons and effects of specific events. Also, people's interest is directly proportional to intensity of a specific event and its effects on people's life. As we know millions of tweets are tweeted everyday so it is impossible to extract topics manually. Twitter has hashtag information to follow the trending topics and frequency of tweets per hashtag can give us some information about popularity of a hashtag. But, hashtag is a user generated string and can lead to many topics or sometimes irrelevant information related to one specific topic. So, we used time-series topic popularity concept to find most of the useful information from twitter's raw data.

\section{Making Popularity Graphs}
The goal of this experiment is to extract topic trends in tweets data efficiently and analyzed visually. The first step towards our goal is to clean the tweets as much as possible in pre-processing, then we use hashtag pooling to make our documents relatively bigger in size as compared to single tweets. Next step is to apply LDA on hashtag pooled tweets data and extract the topics distribution and top words for each topic which convey the meaning of that specific topic. Before the inference, we apply day-hashtag pooling so that we could be able to track the topic trend on time series graph. Inference in this case, is estimating the total number of tweets belong to each topic in each document. As day-hashtag pooling is applied so estimated number of tweets $N_{dk}$ can be calculated using Equation (3.1). The final step in the proposed method is to make graphs of estimated tweets along y-axis and time series along x-axis. In this method we also extract the top words of topics and top hashtags contributing in each topic for each document.

\section{Experiment Procedure(unfinished) }
To meet the expectation of this experiment which is trends analysis from twitter data. We used ``Twitter" dataset. Usually tweets data is very messy so some preprocessing was desirable as a first step for cleaning of this data. So, we removed stop words (the, a, an, in and more) using \textit{nltk.corpus}\footnote{\url{http://www.nltk.org/api/nltk.corpus.html}} python package, special characters (\$, @, \%, \& and more), URLs, and words having only two characters because mostly two characters words do not have concrete meaning(up, vs, ha, RT and more). Next step applied on this dataset was hashtag pooling and after applying it we got 275,836 hashtag pooled documents. Then LDA implementing the stochastic variational Bayesian method of \cite{mimno2012sparse} in Java with 1000 number of topics, 1000 docs per batch also known as mini-batch size and 1000 number of iteration was trained on hashtag pooled documents. The experiment environment was Ubuntu 16.04 for the OS, 2 Intel 8on E5-2630 (2.40 GHz) 8 cores for the CPU, and Python and Java for the implementation. The training of LDA took one hour 17 minutes and 54 seconds and for the inference part the time for calculating $\theta_{dk}$ of documents for topics was just 16 minutes and 20 seconds.