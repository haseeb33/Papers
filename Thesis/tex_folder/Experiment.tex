\chapter{Experiment}
The experimental process started with collecting and preparing the datasets. Then appropriate configurations for DTM and LDA models were selected. After training both models with one dataset set at a time, we extracted topics-word distributions and word probability. These word probabilities were used for computing the JS divergence from which we made the JS similarity-based graphs. We computed the overall correlation and time-series correlation for one of the datasets in one of the configurations to try to extract the DTM time-series topic distributions from the LDA topics. We also plotted population graphs from this configuration's inference part to compare it with the DTM topics.

\section{Datasets}
Three different datasets were used in this experimental procedure.

\textbf{NeurIPS}: This dataset consists of research papers from the conference of neural information processing systems(NeurIPS formally known as NIPS) from 1987, when the conference first started, to 2017. There are 7242 documents in this dataset with three unrecognizable, so a total of 7239 research papers were part of the dataset used in training the DTM and LDA.  In the preprocessing, we removed stop words (e.g., the, a, an, in) using the \textit{nltk.corpus}\footnote{\url{http://www.nltk.org/api/nltk.corpus.html}} python package, special characters (e.g., \$, @, \%, \&), URLs, and words having only two characters because most two characters words do not have concrete meanings (e.g, up, vs, ha).

\textbf{Twitter}: The second dataset is the Tweets2011 dataset of more than three million English tweets sampled between January 23 to February 8, 2011. As the original dataset consists of all the publicly available tweets in that period which means that the tweets are in many languages, we used the Python library \textit{langdetect}\footnote{\url{https://pypi.org/project/langdetect/}} to extract the English tweets. Usually, a tweet is a messy piece of text, so some preprocessing is desirable as the first step in cleaning this data.
We therefore removed stop words, usernames, URLs, special characters, and two-letter words. The next step was day-hashtag pooling. We obtained 408,200 documents after this pooling, which became part of the training dataset of the DTM and LDA.

\textbf{News}: This dataset\footnote{\url{https://components.one/datasets/all-the-news-articles-dataset/}} consists of 204,135 news articles from 18 American publications and each row has columns "id", "title", "author", "date", "content", "year", "month", "publication", "category", "digital", "section", and "url". Each row represents one single news article. Some of the entries may have a NULL value for this article. There are 191,530 articles that have date information and also the distribution of articles over the years is sparse. We therefore selected articles from 2016 and 2017, totaling 95,997 and 75,034 respectively. Thus, a total of 171,031 news articles were divided into 24 time slices based on the month-year parameter for DTM training and the inference of LDA. The same preprocessing steps were applied to this dataset as mentioned above for the other datasets.

\section{Models Configuration}
\textbf{LDA} implementing the stochastic variational Bayesian method of \cite{mimno2012sparse} in Java with three different numbers of topics $K$, 1000 docs per batch (also known as mini-batch size), and 1000 iterations was trained with the above-mentioned datasets one at a time.

\textbf{DTM} was implemented using the gensim.model.wrappers with DTM implementation \footnote{\url{https://github.com/magsilva/dtm}} in C and C++. We trained the DTM on three different numbers of topic configurations with each dataset.

\textbf{Topics:} We selected three values (30, 60, and 90) for the hyperparameter "number of topics", denoted as $K$. We trained the DTM and LDA with one dataset at a time and with one of the $K$ values.

The experiment environment was Ubuntu 16.04 for the operating system, two Intel 80n E5-2630 (2.40 GHz) with eight cores for the central processing unit, and Python and Java for the LDA implementation and Python and C/C++ for the DTM implementation.
